# 示例与教程

<cite>
**本文档引用的文件**   
- [GR00T_inference.ipynb](file://getting_started/GR00T_inference.ipynb)
- [finetune_BEHAVIOR.sh](file://examples/BEHAVIOR/finetune_BEHAVIOR.sh)
- [finetune_g1.sh](file://examples/GR00T-WholeBodyControl/finetune_g1.sh)
- [finetune_libero_10.sh](file://examples/LIBERO/finetune_libero_10.sh)
- [finetune_libero_goal.sh](file://examples/LIBERO/finetune_libero_goal.sh)
- [finetune_libero_object.sh](file://examples/LIBERO/finetune_libero_object.sh)
- [finetune_libero_spatial.sh](file://examples/LIBERO/finetune_libero_spatial.sh)
- [finetune_so100.sh](file://examples/SO100/finetune_so100.sh)
- [finetune_bridge.sh](file://examples/SimplerEnv/finetune_bridge.sh)
- [finetune_fractal.sh](file://examples/SimplerEnv/finetune_fractal.sh)
- [modality.json](file://examples/LIBERO/modality.json)
- [modality.json](file://examples/SO100/modality.json)
- [modality.json](file://examples/SimplerEnv/bridge_modality.json)
- [modality.json](file://examples/SimplerEnv/fractal_modality.json)
- [modality_config.py](file://examples/PointNav/modality_config.py)
- [so100_config.py](file://examples/SO100/so100_config.py)
</cite>

## 目录
1. [简介](#简介)
2. [环境配置与脚本详解](#环境配置与脚本详解)
3. [完整教程：从数据准备到模型推理](#完整教程从数据准备到模型推理)
4. [高级配置选项](#高级配置选项)
5. [故障排除指南](#故障排除指南)
6. [性能优化技巧](#性能优化技巧)

## 简介
本文档旨在为Isaac-GR00T项目提供全面的示例与教程，重点介绍如何使用不同环境（如BEHAVIOR、LIBERO、SO100、SimplerEnv）的配置和脚本。文档将详细说明从数据准备到模型推理的完整工作流程，并为初学者提供循序渐进的指南，同时包含经验丰富的开发者可能需要的高级配置选项。所有内容均基于代码库中的实际文件和一致术语。

## 环境配置与脚本详解
本节将详细介绍如何配置和使用BEHAVIOR、LIBERO、SO100和SimplerEnv等不同环境的脚本。

### BEHAVIOR环境
BEHAVIOR环境用于评估行为1k任务。首先，需要下载转换后的BEHAVIOR数据集：
```bash
huggingface-cli download nvidia/PhysicalAI-Robotics-GR00T-X-Embodiment-Sim \
  --repo-type dataset \
  --include "sim_behavior_r1_pro.*" \
  --local-dir $HOME/gr00t_dataset
```
然后，通过运行以下脚本启动训练：
```bash
uv run bash examples/BEHAVIOR/finetune_BEHAVIOR.sh
```
注意使用`BEHAVIOR_R1_PRO`作为embodiment标签。

**Section sources**
- [finetune_BEHAVIOR.sh](file://examples/BEHAVIOR/finetune_BEHAVIOR.sh)
- [README.md](file://examples/BEHAVIOR/README.md)

### LIBERO环境
LIBERO环境包括多个任务套件，如Spatial、Object、Goal和10 Long。以LIBERO 10为例，首先下载数据集：
```bash
huggingface-cli download \
    --repo-type dataset IPEC-COMMUNITY/libero_10_no_noops_1.0.0_lerobot \
    --local-dir examples/LIBERO/libero_10_no_noops_1.0.0_lerobot/
```
复制模态配置文件并运行微调脚本：
```bash
cp -r examples/LIBERO/modality.json examples/LIBERO/libero_10_no_noops_1.0.0_lerobot/meta/
uv run bash examples/LIBERO/finetune_libero_10.sh
```
其他任务套件（如LIBERO goal、object、spatial）的配置类似，只需替换相应的数据集和脚本。

**Section sources**
- [finetune_libero_10.sh](file://examples/LIBERO/finetune_libero_10.sh)
- [finetune_libero_goal.sh](file://examples/LIBERO/finetune_libero_goal.sh)
- [finetune_libero_object.sh](file://examples/LIBERO/finetune_libero_object.sh)
- [finetune_libero_spatial.sh](file://examples/LIBERO/finetune_libero_spatial.sh)
- [modality.json](file://examples/LIBERO/modality.json)
- [README.md](file://examples/LIBERO/README.md)

### SO100环境
SO100环境用于在真实机器人上评估模型。首先处理数据集：
```bash
uv run python scripts/lerobot_conversion/convert_v3_to_v2.py --repo-id izuluaga/finish_sandwich \
  --root examples/SO100/finish_sandwich_lerobot
```
然后运行微调脚本：
```bash
uv run bash examples/SO100/finetune_so100.sh
```
最后，通过以下命令进行开环评估：
```bash
uv run python gr00t/eval/open_loop_eval.py \
  --dataset-path examples/SO100/finish_sandwich_lerobot \
  --embodiment-tag NEW_EMBODIMENT \
  --model-path /tmp/so100_finetune/checkpoint-10000 \
  --traj-ids 0 \
  --action-horizon 16 \
  --steps 400
```

**Section sources**
- [finetune_so100.sh](file://examples/SO100/finetune_so100.sh)
- [modality.json](file://examples/SO100/modality.json)
- [so100_config.py](file://examples/SO100/so100_config.py)
- [README.md](file://examples/SO100/README.md)

### SimplerEnv环境
SimplerEnv环境用于在模拟中评估真实世界机器人的策略。以SimplerEnv bridge为例，首先下载数据集：
```bash
huggingface-cli download \
    --repo-type dataset IPEC-COMMUNITY/bridge_orig_lerobot \
    --local-dir examples/SimplerEnv/bridge_orig_lerobot/
```
复制模态配置文件并运行微调脚本：
```bash
cp -r examples/SimplerEnv/bridge_modality.json examples/SimplerEnv/bridge_orig_lerobot/meta/modality.json
uv run bash examples/SimplerEnv/finetune_bridge.sh
```
对于SimplerEnv fractal，步骤类似，只需替换相应的数据集和脚本。

**Section sources**
- [finetune_bridge.sh](file://examples/SimplerEnv/finetune_bridge.sh)
- [finetune_fractal.sh](file://examples/SimplerEnv/finetune_fractal.sh)
- [bridge_modality.json](file://examples/SimplerEnv/bridge_modality.json)
- [fractal_modality.json](file://examples/SimplerEnv/fractal_modality.json)
- [README.md](file://examples/SimplerEnv/README.md)

## 完整教程：从数据准备到模型推理
本节将提供来自`getting_started/GR00T_inference.ipynb`的完整教程，展示从数据准备到模型推理的完整工作流程。

### 加载预训练策略
策略模型的加载方式与任何其他Hugging Face模型类似。首先导入必要的库：
```python
import os
import torch
import gr00t
from gr00t.data.dataset.lerobot_episode_loader import LeRobotEpisodeLoader
from gr00t.data.dataset.sharded_single_step_dataset import extract_step_data
from gr00t.data.embodiment_tags import EmbodimentTag
from gr00t.policy.gr00t_policy import Gr00tPolicy
```
然后加载预训练策略：
```python
MODEL_PATH = "nvidia/GR00T-N1.6-3B"
REPO_PATH = os.path.dirname(os.path.dirname(gr00t.__file__))
DATASET_PATH = os.path.join(REPO_PATH, "demo_data/gr1.PickNPlace")
EMBODIMENT_TAG = "gr1"
device = "cuda" if torch.cuda.is_available() else "cpu"

policy = Gr00tPolicy(
    model_path=MODEL_PATH,
    embodiment_tag=EmbodimentTag(EMBODIMENT_TAG),
    device=device,
    strict=True,
)
```

### 加载数据集
首先检查用于预训练`Gr00TPolicy`模型的embodiment标签：
```python
modality_config = policy.get_modality_config()
print(modality_config.keys())
for key, value in modality_config.items():
    if isinstance(value, np.ndarray):
        print(key, value.shape)
    else:
        print(key, value)
```
然后创建数据集：
```python
dataset = LeRobotEpisodeLoader(
    dataset_path=DATASET_PATH,
    modality_configs=modality_config,
    video_backend="torchcodec",
    video_backend_kwargs=None,
)
```
最后，提取并可视化单个数据：
```python
episode_data = dataset[0]
step_data = extract_step_data(
    episode_data, step_index=0, modality_configs=modality_config, embodiment_tag=EmbodimentTag(EMBODIMENT_TAG), allow_padding=False
)
```

**Section sources**
- [GR00T_inference.ipynb](file://getting_started/GR00T_inference.ipynb)

## 高级配置选项
本节将介绍一些高级配置选项，以帮助开发者更好地定制和优化模型。

### 模态配置
模态配置定义了模型使用的字典中的键（如`action`、`state`、`annotation`、`video`）。可以通过`get_modality_config()`方法获取当前的模态配置，并根据需要进行修改。

### 数据增强
在数据加载过程中，可以应用一系列变换来增强数据。这些变换可以在`modality_transform`中定义，以提高模型的泛化能力。

### 自定义embodiment标签
对于新的机器人形态，可以创建自定义的embodiment标签，并在训练和推理过程中使用。这需要在`embodiment_tags.py`中定义新的标签，并确保在配置文件中正确引用。

**Section sources**
- [modality.json](file://examples/LIBERO/modality.json)
- [modality.json](file://examples/SO100/modality.json)
- [modality.json](file://examples/SimplerEnv/bridge_modality.json)
- [modality.json](file://examples/SimplerEnv/fractal_modality.json)
- [modality_config.py](file://examples/PointNav/modality_config.py)

## 故障排除指南
本节将提供常见问题及其解决方案的故障排除指南。

### 数据集加载失败
如果在加载数据集时遇到问题，请确保数据集路径正确，并且数据集文件完整。此外，检查`modality.json`文件是否已正确复制到数据集的`meta`目录中。

### 训练过程中出现内存不足
如果在训练过程中出现内存不足的错误，可以尝试减少批量大小或使用梯度累积。此外，确保GPU有足够的显存来处理当前的模型和数据。

### 推理结果不理想
如果推理结果不理想，可以尝试调整模型的超参数，如学习率、批量大小等。此外，检查输入数据的预处理是否正确，并确保使用了正确的embodiment标签。

**Section sources**
- [README.md](file://examples/BEHAVIOR/README.md)
- [README.md](file://examples/LIBERO/README.md)
- [README.md](file://examples/SO100/README.md)
- [README.md](file://examples/SimplerEnv/README.md)

## 性能优化技巧
本节将介绍一些性能优化技巧，以提高模型的训练和推理效率。

### 使用混合精度训练
混合精度训练可以显著减少训练时间和显存使用。可以通过在训练脚本中启用`--fp16`或`--bf16`选项来实现。

### 优化数据加载
优化数据加载过程可以减少I/O瓶颈。可以使用多线程数据加载器，并确保数据集文件存储在高速存储设备上。

### 模型剪枝和量化
模型剪枝和量化可以减少模型的大小和计算复杂度，从而提高推理速度。这些技术可以在模型训练完成后应用。

**Section sources**
- [finetune_BEHAVIOR.sh](file://examples/BEHAVIOR/finetune_BEHAVIOR.sh)
- [finetune_libero_10.sh](file://examples/LIBERO/finetune_libero_10.sh)
- [finetune_so100.sh](file://examples/SO100/finetune_so100.sh)
- [finetune_bridge.sh](file://examples/SimplerEnv/finetune_bridge.sh)
- [finetune_fractal.sh](file://examples/SimplerEnv/finetune_fractal.sh)